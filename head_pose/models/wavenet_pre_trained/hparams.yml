batch_size: 16
n_epochs: 300
patience: 30 #number of epochs wait until improvement
clipping: true #apply gradient clipping
min_delta: 0.01 #min delta of improvement for early stopping
start_fold: 0 #fold will be ran
verbose_train: true
num_workers: 2
model_name: wavenet # CHANGE WHEN CREATE A NEW MODEL
checkpoint_path: ./data/model_weights/wavenet/checkpoint # CHANGE WHEN CREATE A NEW MODEL
model_path: ./data/model_weights/wavenet # CHANGE WHEN CREATE A NEW MODEL
model:
  n_channels: 1
  n_samples: 15000
  wavenet_dilation: [2,4,8,16,32]
  layer_feature_maps: [8,16] #feature maps for STEM layer, last layer defines number of wavenet filters
  dropout_rate: 0.1
  pool_size: 2
  kernel_size: 3
optimizer_name: Adam
optimizer_hparams:
  lr: 0.0001
  weight_decay: 0.0
#scheduler_name: CosineAnnealingLR
#scheduler_hparams:
#  T_max: 15
#  eta_min: 1.0e-09
#  last_epoch: -1
#  verbose: true
scheduler_name: ReduceLROnPlateau
scheduler_hparams:
  patience: 3
  threshold: 0.02
  verbose: true
